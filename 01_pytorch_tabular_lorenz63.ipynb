{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/pytorch_tabular/models/mixture_density/mdn.py:25: UserWarning: Wandb not installed. WandbLogger will not work.\n",
      "  warnings.warn(\"Wandb not installed. WandbLogger will not work.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lorenz system\n",
    "def lorenz63(t, X):\n",
    "    u, v, w = X\n",
    "    up = -10*(u - v)\n",
    "    vp = 28*u - v - u*w\n",
    "    wp = -2.667*w + u*v\n",
    "    return up, vp, wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation params\n",
    "# max time, number of data points\n",
    "tmax, n = 100, 1000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 17:37:05,760 - {pytorch_tabular.tabular_model:102} - INFO - Experiment Tracking is turned off\n"
     ]
    }
   ],
   "source": [
    "# PyTorch model\n",
    "# Lags at 1-2-3-4-5 as feature\n",
    "data_config = DataConfig(\n",
    "    target=[\"x\", \"y\", \"z\"],\n",
    "    continuous_cols=['x_1', 'y_1', 'z_1', 'x_2', 'y_2', 'z_2', 'x_3', 'y_3',\n",
    "       'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5'],\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,\n",
    "    batch_size=64,\n",
    "    max_epochs=100,\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "# Three layer MLP with LeakyReLU activation functions\n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "    task=\"regression\",\n",
    "    layers=\"32-16-16\",\n",
    "    activation=\"LeakyReLU\",\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "2023-04-15 17:38:47,588 - {pytorch_tabular.tabular_model:465} - INFO - Preparing the DataLoaders\n",
      "2023-04-15 17:38:47,590 - {pytorch_tabular.tabular_datamodule:286} - INFO - Setting up the datamodule for regression task\n",
      "2023-04-15 17:38:47,632 - {pytorch_tabular.tabular_model:508} - INFO - Preparing the Model: CategoryEmbeddingModel\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/pytorch_tabular/models/base_model.py:126: UserWarning: Wandb is not installed. Please install wandb to log logits. You can install wandb using pip install wandb or install PyTorch Tabular using pip install pytorch-tabular[all]\n",
      "  warnings.warn(\n",
      "2023-04-15 17:38:47,790 - {pytorch_tabular.tabular_model:264} - INFO - Preparing the Trainer\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-04-15 17:38:47,870 - {pytorch_tabular.tabular_model:558} - INFO - Auto LR Find Started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6350e36153544245a34470fb5ccac2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.04365158322401657\n",
      "Restoring states from the checkpoint path at /workspaces/pytorch-examples/.lr_find_c69389d8-e2ce-485c-8406-9f4b0d7e7e81.ckpt\n",
      "Restored all states from the checkpoint file at /workspaces/pytorch-examples/.lr_find_c69389d8-e2ce-485c-8406-9f4b0d7e7e81.ckpt\n",
      "2023-04-15 17:38:49,584 - {pytorch_tabular.tabular_model:560} - INFO - Suggested LR: 0.04365158322401657. For plot and detailed analysis, use `find_learning_rate` method.\n",
      "2023-04-15 17:38:49,586 - {pytorch_tabular.tabular_model:566} - INFO - Training Started\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ CategoryEmbeddingBackbone │  1.3 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer          │     30 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ head             │ LinearHead                │     51 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ MSELoss                   │      0 │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ CategoryEmbeddingBackbone │  1.3 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer          │     30 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ head             │ LinearHead                │     51 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ MSELoss                   │      0 │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.4 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.4 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.4 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.4 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e958ddfb26446dc98a585fe7a459ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and predict for 10 different starting conditions\n",
    "for _ in range(10):\n",
    "    # Generate random starting point\n",
    "    u0 = np.random.randn()\n",
    "    v0 = np.random.randn()\n",
    "    w0 = np.random.randn()\n",
    "\n",
    "    # Solve system\n",
    "    soln = solve_ivp(lorenz63, (0, tmax), (u0, v0, w0), dense_output=True)\n",
    "    t = np.linspace(0, tmax, n)\n",
    "    x, y, z = soln.sol(t)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    df = pd.DataFrame(np.concatenate([x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)], axis=1), columns=[\"x\", \"y\", \"z\"])\n",
    "    for i in range(5):\n",
    "        df[f\"x_{i+1}\"] = df.x.shift(i+1)\n",
    "        df[f\"y_{i+1}\"] = df.y.shift(i+1)\n",
    "        df[f\"z_{i+1}\"] = df.z.shift(i+1)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Split data as train-valid-test\n",
    "    traindata, validdata, testdata = df.iloc[0:500,:], df.iloc[501:800,:], df.iloc[801:,:]\n",
    "    \n",
    "    # Fit - predict\n",
    "    tabular_model.fit(train=traindata, validation=validdata)\n",
    "    result = tabular_model.evaluate(testdata)\n",
    "    pred_df = tabular_model.predict(testdata)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(3, 1)\n",
    "    ax[0].set_title(f\"Starting conditions: {np.round(u0, 6)},{np.round(v0, 6)},{np.round(w0, 6)}\")\n",
    "    ax[0].plot(pred_df['x'], label=\"x\")\n",
    "    ax[0].plot(pred_df['x_prediction'], label=\"pred\")\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(pred_df['y'], label=\"y\")\n",
    "    ax[1].plot(pred_df['y_prediction'], label=\"pred\")\n",
    "    ax[1].legend()\n",
    "    ax[2].plot(pred_df['z'], label=\"z\")\n",
    "    ax[2].plot(pred_df['z_prediction'], label=\"pred\")\n",
    "    ax[2].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
